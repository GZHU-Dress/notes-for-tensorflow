# GoogleNet
***
## 概述

Inception V1 有22层深，比 AlexNet 的8层和 VGGNet 的19层还要更深。其最大的特点就是控制了计算量和参数量的同时，获得了非常好的分类性能。

其降低参数的目的有两点：第一，参数越多模型越庞大，需要提供的数据量就越大，而目前高质量的数据非常珍贵；第二，参数越多需要的计算资源也就越多。

Inception V1 参数少但是效果好的原因除了模型层数更深、表达能力更强外，还有两点：一是去除了最后的全连接层，用全局平均池化层（即将图片尺寸变为1x1）来取代它。全连接层几乎占据了 AlexNet 和 VGGNet 中90%的参数量，而且会引起过拟合，去除全连接层后的模型训练更快并且减轻了过拟合。二是 Inception V1 中精心设计的 Inception Module 提高了参数的利用效率。

## 结构图

### Inception Module 结构图

![](https://hackathonprojects.files.wordpress.com/2016/09/inception_implement.png)

1x1 的卷积是一个非常优秀的结构，它可以跨通道组织信息，提高网络表达能力，同时可以对输出通道升维和降维。

可以看到其四个通道都用了1x1卷积，来进行低成本（计算量比3x3小很多）的跨通道的特征变换。第二个分支先用了1x1卷积，然后连接3x3卷积，相当于进行了两次特征变换。第三个分支类似，先是1x1卷积，再是5x5卷积。最后一个分支则是3x3的最大池化后直接使用1x1卷积。

可以发现，1x1卷积在各个分支中都有使用，这是因为1x1卷积的性价比很高，用很小的计算两就能增加一层特征变换和非线性化。

四个分支最后通过一个聚合操作合并（在输出通道数这个维度上聚合）。总的来说，Inception Module 中包含了3种不同尺寸的卷积和一个最大池化，增加了网络对不同尺度的适应性。Inception Net 主要目的就是找到最优的系数结构单元（即 Inception Module）

### GoogleNet 结构图

![](https://adeshpande3.github.io/assets/GoogleNet.gif)

## 原理

### Hebbian 原理
神经反射活动的持续与重复会导致神经元连接稳定性的持久提升，当两个神经元细胞 A 和 B 距离很近，并且 A 参与了对 B 重复、持续的兴奋，那么某些代谢变化会导致 A 将作为能使 B 兴奋的细胞。总结一下即“一起发射的神经元会连在一起”，学习过程中的刺激会使神经元间的突触强度增加。

受其原理的启发，如果数据集的概率分布可以被一个很大很系数的神经网络表达，那么每个构筑这个网络的最佳方法是逐层构筑网络：将上一层高度相关（correlated）的节点聚类，并将聚类出来的每一个小簇（cluster）连接到一起，这个相关性高的节点应该被连接在一起的结论，即是从神经网络的角度对 Hebbian 原理有效性的证明。因此一个“好”的稀疏结构应该是符合 Hebbian 原理的，我们应该把相关性高的一簇神经元节点连接在一起，这就是为什么1x1卷积这么频繁地被应用到 Inception Net 中的原因。1x1卷积所连接的节点的相关性是最高的，而稍微大一点尺寸的卷积所连接的节点相关性也很高，因此可以用一些大尺寸的卷积，增加多样性（diversity）。最后 Inception Module 通过四个分支中不同尺寸的小型卷积将相关性很高的节点连接一起，就完成了其设计初衷，构建出了很高效的符合 Hebbian 原理的稀疏结构。

### 卷积核大小选择的原则
在 Inception Module 中，通常1x1卷积的比例最高，3x3卷积和5x5卷积稍低。而整个网络中，会有多个堆叠的 Inception Module，我们希望靠后的 Inception Module 可以捕捉更高阶的抽象特征，因此靠后的 Inception Module 的卷积空间集中度应该逐渐降低，这样可以捕获更大面积的特征。因此，越靠后的 Inception Module中，3x3和5x5这两个大面积的卷积核的占比应该更多。

Inception Net 有22层深，除了最后一层的输出，其中间节点的分类效果也很好。因此在其中还使用到了辅助分类节点（auxiliary classifiers），即将中间某一层的输出用作分类，并按一个较小的权重（0.3）加到最终分类结果中。这样相当于做了模型融合，同时给网络增加了反向传播的梯度信号，也提供了额外的正则化，对于整个 Inception Net 的训练很有裨益。

Inception V2 学习了 VGGNet，用两个3x3的卷积替代5x5的大卷积（用以降低参数量并减轻过拟合）。还提出了著名的 Batch Normalization（简称BN）方法。它是一种非常有效的正则化方法，可以让大型卷积网络的训练速度加快很多倍，同时收敛后的分类准确率也可以得到大幅度提高。
